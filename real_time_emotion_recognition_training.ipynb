{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1732825,"sourceType":"datasetVersion","datasetId":1028436}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define constants\nimg_height, img_width = 48, 48\nnum_classes = 7\nbatch_size = 256\nepochs = 5\n\n# Custom dataset class\nclass FERDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.emotion_labels = os.listdir(root_dir)\n        self.image_paths = []\n        self.labels = []\n\n        # Load image paths and corresponding labels\n        for label in self.emotion_labels:\n            label_dir = os.path.join(root_dir, label)\n            for img_file in os.listdir(label_dir):\n                self.image_paths.append(os.path.join(label_dir, img_file))\n                self.labels.append(self.emotion_labels.index(label))\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('L')  # Load image and convert to grayscale\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_height, img_width)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Normalize for grayscale\n])\n\n# Load datasets\ntrain_dir = '/kaggle/input/emotion-detection-fer/train'  # Update to your training folder path\ntest_dir = '/kaggle/input/emotion-detection-fer/test'      # Update to your testing folder path\n\ntrain_dataset = FERDataset(train_dir, transform=transform)\ntest_dataset = FERDataset(test_dir, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the CNN model\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(128 * (img_height // 8) * (img_width // 8), 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(-1, 128 * (img_height // 8) * (img_width // 8))\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = CNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    pbar = tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{epochs}')\n    total_loss = 0\n\n    for images, labels in pbar:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n\n    train_losses.append(total_loss / len(train_loader))\n\n    # Validate the model\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n    test_losses.append(total_loss / len(test_loader))\n    print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n\n# Plot training and validation loss\nplt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\nplt.plot(range(1, epochs + 1), test_losses, label='Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Save the trained model\ntorch.save(model.state_dict(), 'emotion_recognition_model.pth')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T21:52:49.980441Z","iopub.execute_input":"2024-10-30T21:52:49.980881Z","iopub.status.idle":"2024-10-30T21:58:42.648922Z","shell.execute_reply.started":"2024-10-30T21:52:49.980841Z","shell.execute_reply":"2024-10-30T21:58:42.647575Z"},"trusted":true},"execution_count":null,"outputs":[]}]}